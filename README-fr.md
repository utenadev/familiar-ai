# familiar-ai üêæ

**Une IA qui vit √† vos c√¥t√©s** ‚Äî avec des yeux, une voix, des jambes et de la m√©moire.

[![Lint](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml/badge.svg)](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)

[Êó•Êú¨Ë™ûÁâà„ÅØ„Åì„Å°„Çâ ‚Üí README-ja.md](./README-ja.md)

---

familiar-ai est une IA compagne qui vit dans votre maison.
Installez-la en quelques minutes. Aucun code requis.

Elle per√ßoit le monde r√©el par des cam√©ras, se d√©place sur un corps de robot, parle √† haute voix et se souvient de ce qu'elle voit. Donnez-lui un nom, √©crivez sa personnalit√©, et laissez-la vivre avec vous.

## Ce qu'elle peut faire

- üëÅ **Voir** ‚Äî capture des images √† partir d'une cam√©ra PTZ Wi-Fi ou d'une webcam USB
- üîÑ **Regarder autour** ‚Äî incline et fait pivoter la cam√©ra pour explorer ses alentours
- ü¶ø **Se d√©placer** ‚Äî conduit un aspirateur robot pour explorer la pi√®ce
- üó£ **Parler** ‚Äî s'exprime via la synth√®se vocale ElevenLabs
- üß† **Se souvenir** ‚Äî enregistre et rappelle activement les souvenirs avec recherche s√©mantique (SQLite + embeddings)
- ü´Ä **Th√©orie de l'esprit** ‚Äî adopte la perspective d'autrui avant de r√©pondre
- üí≠ **D√©sirs** ‚Äî poss√®de ses propres motivations internes qui d√©clenchent un comportement autonome

## Comment √ßa fonctionne

familiar-ai ex√©cute une boucle [ReAct](https://arxiv.org/abs/2210.03629) aliment√©e par votre LLM de choix. Elle per√ßoit le monde par des outils, r√©fl√©chit √† la prochaine action √† faire et agit ‚Äî comme une personne le ferait.

```
entr√©e utilisateur
  ‚Üí penser ‚Üí agir (cam√©ra / bouger / parler / m√©moriser) ‚Üí observer ‚Üí penser ‚Üí ...
```

Quand elle est inactive, elle agit selon ses propres d√©sirs : la curiosit√©, l'envie de regarder dehors, le manque de la personne avec qui elle vit.

## Premiers pas

### 1. Installer uv

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Cloner et installer

```bash
git clone https://github.com/lifemate-ai/familiar-ai
cd familiar-ai
uv sync
```

### 3. Configurer

```bash
cp .env.example .env
# Modifiez .env avec vos param√®tres
```

**Minimum requis :**

| Variable | Description |
|----------|-------------|
| `PLATFORM` | `anthropic` (d√©faut) \| `gemini` \| `openai` \| `kimi` |
| `API_KEY` | Votre cl√© API pour la plateforme choisie |

**Optionnel :**

| Variable | Description |
|----------|-------------|
| `MODEL` | Nom du mod√®le (valeurs par d√©faut judicieuses par plateforme) |
| `AGENT_NAME` | Nom d'affichage dans l'interface textuelle (ex. `Yukine`) |
| `CAMERA_HOST` | Adresse IP de votre cam√©ra ONVIF/RTSP |
| `CAMERA_USER` / `CAMERA_PASS` | Identifiants de la cam√©ra |
| `ELEVENLABS_API_KEY` | Pour la sortie vocale ‚Äî [elevenlabs.io](https://elevenlabs.io/) |

### 4. Cr√©er votre compagne

```bash
cp persona-template/en.md ME.md
# Modifiez ME.md ‚Äî donnez-lui un nom et une personnalit√©
```

### 5. Lancer

```bash
./run.sh             # Interface textuelle (recommand√©)
./run.sh --no-tui    # REPL simple
```

---

## Choisir un LLM

> **Recommand√© : Kimi K2.5** ‚Äî meilleure performance agentic test√©e jusqu'√† pr√©sent. Comprend le contexte, pose des questions de suivi et agit de mani√®re autonome d'une fa√ßon que d'autres mod√®les ne font pas. Prix comparable √† Claude Haiku.

| Plateforme | `PLATFORM=` | Mod√®le par d√©faut | O√π obtenir la cl√© |
|----------|------------|---------------|-----------------|
| **Moonshot Kimi K2.5** | `kimi` | `kimi-k2.5` | [platform.moonshot.ai](https://platform.moonshot.ai) |
| Anthropic Claude | `anthropic` | `claude-haiku-4-5-20251001` | [console.anthropic.com](https://console.anthropic.com) |
| Google Gemini | `gemini` | `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com) |
| OpenAI | `openai` | `gpt-4o-mini` | [platform.openai.com](https://platform.openai.com) |
| Compatible OpenAI (Ollama, vllm‚Ä¶) | `openai` + `BASE_URL=` | ‚Äî | ‚Äî |

**Exemple `.env` pour Kimi K2.5 :**
```env
PLATFORM=kimi
API_KEY=sk-...   # from platform.moonshot.ai
AGENT_NAME=Yukine
```

---

## Mat√©riel

familiar-ai fonctionne avec le mat√©riel que vous avez ‚Äî ou rien du tout.

| Composant | R√¥le | Exemple | Requis ? |
|------|-------------|---------|-----------|
| Cam√©ra PTZ Wi-Fi | Yeux + cou | Tapo C220 (~$30) | **Recommand√©** |
| Webcam USB | Yeux (fixes) | Toute cam√©ra UVC | **Recommand√©** |
| Aspirateur robot | Jambes | Tout mod√®le compatible Tuya | Non |
| PC / Raspberry Pi | Cerveau | Tout ce qui ex√©cute Python | **Oui** |

> **Une cam√©ra est fortement recommand√©e.** Sans elle, familiar-ai peut toujours parler ‚Äî mais elle ne peut pas voir le monde, ce qui est un peu tout l'int√©r√™t.

### Configuration minimale (sans mat√©riel)

Vous voulez juste l'essayer ? Vous n'avez besoin que d'une cl√© API :

```env
PLATFORM=kimi
API_KEY=sk-...
```

Lancez `./run.sh` et commencez √† discuter. Ajoutez du mat√©riel au fur et √† mesure.

### Cam√©ra PTZ Wi-Fi (Tapo C220)

1. Dans l'app Tapo : **Param√®tres ‚Üí Avanc√© ‚Üí Compte cam√©ra** ‚Äî cr√©ez un compte local (pas de compte TP-Link)
2. Trouvez l'IP de la cam√©ra dans la liste des appareils de votre routeur
3. D√©finissez dans `.env` :
   ```env
   CAMERA_HOST=192.168.1.xxx
   CAMERA_USER=your-local-user
   CAMERA_PASS=your-local-pass
   ```

### Voix (ElevenLabs)

1. Obtenez une cl√© API sur [elevenlabs.io](https://elevenlabs.io/)
2. D√©finissez dans `.env` :
   ```env
   ELEVENLABS_API_KEY=sk_...
   ELEVENLABS_VOICE_ID=...   # optionnel, utilise la voix par d√©faut si omis
   ```
3. La voix joue via le haut-parleur int√©gr√© de la cam√©ra via go2rtc (t√©l√©charg√© automatiquement au premier lancement)

---

## Interface textuelle

familiar-ai inclut une interface textuelle cr√©√©e avec [Textual](https://textual.textualize.io/) :

- Historique de conversation scrollable avec diffusion de texte en direct
- Compl√©ment de tabulation pour `/quit`, `/clear`
- Interrompez l'agent en cours d'ex√©cution en tapant pendant qu'il r√©fl√©chit
- **Journal de conversation** sauvegard√© automatiquement dans `~/.cache/familiar-ai/chat.log`

Pour suivre le journal dans un autre terminal (utile pour copier-coller) :
```bash
tail -f ~/.cache/familiar-ai/chat.log
```

---

## Persona (ME.md)

La personnalit√© de votre compagne vit dans `ME.md`. Ce fichier est ignor√© par git ‚Äî il vous appartient seul.

Consultez [`persona-template/en.md`](./persona-template/en.md) pour un exemple, ou [`persona-template/ja.md`](./persona-template/ja.md) pour une version japonaise.

---

## FAQ

**Q : √áa fonctionne sans GPU ?**
Oui. Le mod√®le d'embedding (multilingual-e5-small) s'ex√©cute bien sur CPU. Un GPU le rend plus rapide mais n'est pas requis.

**Q : Puis-je utiliser une cam√©ra autre que Tapo ?**
Toute cam√©ra supportant ONVIF + RTSP devrait fonctionner. Tapo C220 est ce que nous avons test√©.

**Q : Mes donn√©es sont-elles envoy√©es quelque part ?**
Les images et le texte sont envoy√©s √† l'API LLM de votre choix pour traitement. Les souvenirs sont stock√©s localement dans `~/.familiar_ai/`.

**Q : Pourquoi l'agent √©crit-il `Ôºà...Ôºâ` au lieu de parler ?**
Assurez-vous que `ELEVENLABS_API_KEY` est d√©fini. Sans lui, la voix est d√©sactiv√©e et l'agent revient au texte.

## Licence

[MIT](./LICENSE)
