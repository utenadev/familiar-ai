# familiar-ai üêæ

**Eine KI, die neben dir lebt** ‚Äî mit Augen, Stimme, Beinen und Ged√§chtnis.

[![Lint](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml/badge.svg)](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)

[English README](./README.md)

---

[![Demo-Video](https://img.youtube.com/vi/PQvQ6M6-Uys/0.jpg)](https://youtube.com/shorts/PQvQ6M6-Uys)

familiar-ai ist ein KI-Begleiter, der in deinem Zuhause lebt.
Richte es in wenigen Minuten ein. Keine Programmierkenntnisse erforderlich.

Es nimmt die reale Welt durch Kameras wahr, bewegt sich auf einem Roboter, spricht laut und erinnert sich an das, was es sieht. Gib ihm einen Namen, schreibe seine Pers√∂nlichkeit auf und lass es bei dir leben.

## Was es kann

- üëÅ **Sehen** ‚Äî erfasst Bilder von einer Wi-Fi-PTZ-Kamera oder USB-Webcam
- üîÑ **Umschauen** ‚Äî schwenkt und neigt die Kamera, um die Umgebung zu erkunden
- ü¶ø **Bewegen** ‚Äî steuert einen Roboterstaubsauger durchs Zimmer
- üó£ **Sprechen** ‚Äî spricht via ElevenLabs TTS
- üß† **Erinnern** ‚Äî speichert und ruft aktiv Erinnerungen mit semantischer Suche ab (SQLite + Embeddings)
- ü´Ä **Theory of Mind** ‚Äî ber√ºcksichtigt die Perspektive des anderen, bevor es antwortet
- üí≠ **W√ºnsche** ‚Äî hat eigene innere Antriebe, die autonomes Verhalten ausl√∂sen

## Wie es funktioniert

familiar-ai f√ºhrt eine [ReAct](https://arxiv.org/abs/2210.03629)-Schleife aus, angetrieben durch dein gew√§hltes LLM. Es nimmt die Welt durch Tools wahr, √ºberlegt sich, was es tun soll, und handelt ‚Äî wie ein Mensch.

```
Benutzereingabe
  ‚Üí √ºberlegen ‚Üí handeln (Kamera / bewegen / sprechen / erinnern) ‚Üí beobachten ‚Üí √ºberlegen ‚Üí ...
```

Im Leerlauf handelt es nach seinen eigenen W√ºnschen: Neugier, den Wunsch nach drau√üen zu schauen, das Vermissen der Person, mit der es lebt.

## Erste Schritte

### 1. Installiere uv

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Installiere ffmpeg

ffmpeg ist **erforderlich** f√ºr die Kamerabilderfassung und Audiowiedergabe.

| OS | Befehl |
|----|--------|
| macOS | `brew install ffmpeg` |
| Ubuntu / Debian | `sudo apt install ffmpeg` |
| Fedora / RHEL | `sudo dnf install ffmpeg` |
| Arch Linux | `sudo pacman -S ffmpeg` |
| Windows | `winget install ffmpeg` ‚Äî oder von [ffmpeg.org](https://ffmpeg.org/download.html) herunterladen und zu PATH hinzuf√ºgen |
| Raspberry Pi | `sudo apt install ffmpeg` |

√úberpr√ºfen: `ffmpeg -version`

### 3. Klone und installiere

```bash
git clone https://github.com/lifemate-ai/familiar-ai
cd familiar-ai
uv sync
```

### 4. Konfiguriere

```bash
cp .env.example .env
# Bearbeite .env mit deinen Einstellungen
```

**Erforderlich:**

| Variable | Beschreibung |
|----------|-------------|
| `PLATFORM` | `anthropic` (Standard) \| `gemini` \| `openai` \| `kimi` |
| `API_KEY` | Dein API-Schl√ºssel f√ºr die gew√§hlte Plattform |

**Optional:**

| Variable | Beschreibung |
|----------|-------------|
| `MODEL` | Modellname (sinnvolle Standard pro Plattform) |
| `AGENT_NAME` | Anzeigename in der TUI (z. B. `Yukine`) |
| `CAMERA_HOST` | IP-Adresse deiner ONVIF/RTSP-Kamera |
| `CAMERA_USER` / `CAMERA_PASS` | Anmeldedaten der Kamera |
| `ELEVENLABS_API_KEY` | F√ºr Sprachausgabe ‚Äî [elevenlabs.io](https://elevenlabs.io/) |

### 5. Erstelle deinen Familiar

```bash
cp persona-template/en.md ME.md
# Bearbeite ME.md ‚Äî gib ihm einen Namen und eine Pers√∂nlichkeit
```

### 6. Starten

```bash
./run.sh             # Textuelle TUI (empfohlen)
./run.sh --no-tui    # Einfache REPL
```

---

## Ein LLM w√§hlen

> **Empfohlen: Kimi K2.5** ‚Äî beste Agent-Performance, die bisher getestet wurde. Bemerkt Kontext, stellt Nachfragen und handelt auf Weise autonom, wie andere Modelle nicht. Preis √§hnlich wie Claude Haiku.

| Plattform | `PLATFORM=` | Standardmodell | Wo man den Schl√ºssel bekommt |
|----------|------------|---------------|-----------------|
| **Moonshot Kimi K2.5** | `kimi` | `kimi-k2.5` | [platform.moonshot.ai](https://platform.moonshot.ai) |
| Anthropic Claude | `anthropic` | `claude-haiku-4-5-20251001` | [console.anthropic.com](https://console.anthropic.com) |
| Google Gemini | `gemini` | `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com) |
| OpenAI | `openai` | `gpt-4o-mini` | [platform.openai.com](https://platform.openai.com) |
| OpenAI-kompatibel (Ollama, vllm‚Ä¶) | `openai` + `BASE_URL=` | ‚Äî | ‚Äî |
| OpenRouter.ai (Mehrfachanbieter) | `openai` + `BASE_URL=https://openrouter.ai/api/v1` | ‚Äî | [openrouter.ai](https://openrouter.ai) |
| **CLI-Tool** (llm, ollama‚Ä¶) | `cli` | (der Befehl) | ‚Äî |

**Kimi K2.5 `.env` Beispiel:**
```env
PLATFORM=kimi
API_KEY=sk-...   # von platform.moonshot.ai
AGENT_NAME=Yukine
```

**Google Gemini `.env` Beispiel:**
```env
PLATFORM=gemini
API_KEY=AIza...   # von aistudio.google.com
MODEL=gemini-2.5-flash  # oder gemini-2.5-pro
AGENT_NAME=Yukine
```

**OpenRouter.ai `.env` Beispiel:**
```env
PLATFORM=openai
BASE_URL=https://openrouter.ai/api/v1
API_KEY=sk-or-...   # von openrouter.ai
MODEL=mistralai/mistral-7b-instruct  # optional
AGENT_NAME=Yukine
```

> **Hinweis:** Um lokale/NVIDIA-Modelle zu deaktivieren, setzen Sie `BASE_URL` nicht auf einen lokalen Endpunkt wie `http://localhost:11434/v1`. Verwenden Sie stattdessen Cloud-Anbieter.

**CLI-Tool `.env` Beispiel:**
```env
PLATFORM=cli
MODEL=llm -m gemma3 {}        # llm CLI (https://llm.datasette.io) ‚Äî {} = Prompt-Argument
# MODEL=ollama run gemma3:27b  # Ollama ‚Äî ohne {}, Prompt via stdin
```

---

## MCP-Server

familiar-ai kann sich mit jedem [MCP (Model Context Protocol)](https://modelcontextprotocol.io) Server verbinden und so externe Speicher, Dateizugriff, Websuche und weitere Tools einbinden.

Konfigurieren Sie Server in `~/.familiar-ai.json` (gleiches Format wie Claude Code):

```json
{
  "mcpServers": {
    "filesystem": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user"]
    },
    "memory": {
      "type": "sse",
      "url": "http://localhost:3000/sse"
    }
  }
}
```

Zwei Transporttypen werden unterst√ºtzt:
- **`stdio`**: startet einen lokalen Unterprozess (`command` + `args`)
- **`sse`**: verbindet sich mit einem HTTP+SSE-Server (`url`)

√úberschreiben Sie den Pfad der Konfigurationsdatei mit `MCP_CONFIG=/path/to/config.json`.

---

## Hardware

familiar-ai funktioniert mit jeder Hardware, die du hast ‚Äî oder mit gar keiner.

| Teil | Funktion | Beispiel | Erforderlich? |
|------|----------|---------|-----------|
| Wi-Fi-PTZ-Kamera | Augen + Nacken | Tapo C220 (~$30) | **Empfohlen** |
| USB-Webcam | Augen (fest) | Jede UVC-Kamera | **Empfohlen** |
| Roboterstaubsauger | Beine | Jedes Tuya-kompatibles Modell | Nein |
| PC / Raspberry Pi | Gehirn | Alles, das Python ausf√ºhrt | **Ja** |

> **Eine Kamera wird dringend empfohlen.** Ohne sie kann familiar-ai zwar sprechen ‚Äî aber es kann die Welt nicht sehen, was ja der ganze Sinn der Sache ist.

### Minimales Setup (keine Hardware)

Du m√∂chtest es nur ausprobieren? Du brauchst nur einen API-Schl√ºssel:

```env
PLATFORM=kimi
API_KEY=sk-...
```

Starte `./run.sh` und fang an zu chatten. F√ºge Hardware sp√§ter hinzu.

### Wi-Fi-PTZ-Kamera (Tapo C220)

1. In der Tapo-App: **Einstellungen ‚Üí Erweitert ‚Üí Kamerakonto** ‚Äî erstelle ein lokales Konto (nicht TP-Link-Konto)
2. Finde die IP-Adresse der Kamera in der Ger√§teliste deines Routers
3. Stelle in `.env` ein:
   ```env
   CAMERA_HOST=192.168.1.xxx
   CAMERA_USER=dein-lokaler-benutzer
   CAMERA_PASS=dein-lokales-passwort
   ```

### Stimme (ElevenLabs)

1. Hole dir einen API-Schl√ºssel auf [elevenlabs.io](https://elevenlabs.io/)
2. Stelle in `.env` ein:
   ```env
   ELEVENLABS_API_KEY=sk_...
   ELEVENLABS_VOICE_ID=...   # optional, verwendet Standardstimme wenn weggelassen
   ```
Es gibt zwei Wiedergabeziele:

#### A) Kameralautsprecher (via go2rtc)

Um Audio √ºber den integrierten Kameralautsprecher abzuspielen, muss [go2rtc](https://github.com/AlexxIT/go2rtc/releases) manuell eingerichtet werden:

1. Lade das Binary von der [Releases-Seite](https://github.com/AlexxIT/go2rtc/releases) herunter:
   - Linux/macOS: `go2rtc_linux_amd64` / `go2rtc_darwin_amd64`
   - **Windows: `go2rtc_win64.exe`**

2. Ablegen und umbenennen:
   ```
   # Linux / macOS
   ~/.cache/embodied-claude/go2rtc/go2rtc          # chmod +x erforderlich

   # Windows
   %USERPROFILE%\.cache\embodied-claude\go2rtc\go2rtc.exe
   ```

3. `go2rtc.yaml` im selben Verzeichnis erstellen:
   ```yaml
   streams:
     tapo_cam:
       - rtsp://YOUR_CAM_USER:YOUR_CAM_PASS@YOUR_CAM_IP/stream1
   ```

4. familiar-ai startet go2rtc automatisch beim Start. Wenn die Kamera bidirektionales Audio (Backchannel) unterst√ºtzt, kommt die Stimme aus dem Kameralautsprecher.

#### B) Lokaler PC-Lautsprecher (Fallback)

Ohne go2rtc oder wenn die Kamera kein Backchannel-Audio unterst√ºtzt, wird auf **mpv** oder **ffplay** zur√ºckgefallen:

| OS | Installation |
|----|-------------|
| macOS | `brew install mpv` |
| Ubuntu / Debian | `sudo apt install mpv` |
| Windows | [mpv.io/installation](https://mpv.io/installation/) ‚Äî herunterladen und zu PATH hinzuf√ºgen, **oder** `winget install ffmpeg` |

> Ohne go2rtc und lokalen Player funktioniert die Sprachgenerierung (ElevenLabs API) weiterhin ‚Äî die Wiedergabe wird lediglich √ºbersprungen.

---

## TUI

familiar-ai enth√§lt eine Terminal-Benutzeroberfl√§che, gebaut mit [Textual](https://textual.textualize.io/):

- Scrollbarer Gespr√§chsverlauf mit Live-Streaming-Text
- Tab-Vervollst√§ndigung f√ºr `/quit`, `/clear`
- Unterbreche den Agent w√§hrend des Denkens, indem du tippst
- **Gespr√§chsprotokoll** wird automatisch in `~/.cache/familiar-ai/chat.log` gespeichert

Um das Protokoll in einem anderen Terminal zu verfolgen (n√ºtzlich zum Kopieren):
```bash
tail -f ~/.cache/familiar-ai/chat.log
```

---

## Pers√∂nlichkeit (ME.md)

Die Pers√∂nlichkeit deines Familiars lebt in `ME.md`. Diese Datei ist gitignoriert ‚Äî sie geh√∂rt dir allein.

Siehe [`persona-template/en.md`](./persona-template/en.md) f√ºr ein Beispiel oder [`persona-template/ja.md`](./persona-template/ja.md) f√ºr eine japanische Version.

---

## H√§ufig gestellte Fragen

**F: Funktioniert es ohne GPU?**
Ja. Das Embedding-Modell (multilingual-e5-small) l√§uft problemlos auf der CPU. Eine GPU macht es schneller, ist aber nicht erforderlich.

**F: Kann ich eine andere Kamera als Tapo verwenden?**
Jede Kamera, die ONVIF + RTSP unterst√ºtzt, sollte funktionieren. Tapo C220 ist das, womit wir getestet haben.

**F: Werden meine Daten irgendwo hingekannt?**
Bilder und Text werden an deine gew√§hlte LLM-API zum Verarbeiten gesendet. Erinnerungen werden lokal in `~/.familiar_ai/` gespeichert.

**F: Warum schreibt der Agent `Ôºà...Ôºâ` statt zu sprechen?**
Stelle sicher, dass `ELEVENLABS_API_KEY` gesetzt ist. Ohne ihn ist Sprache deaktiviert und der Agent f√§llt auf Text zur√ºck.

## Lizenz

[MIT](./LICENSE)
