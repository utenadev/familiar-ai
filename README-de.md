# familiar-ai üêæ

**Eine KI, die neben dir lebt** ‚Äî mit Augen, Stimme, Beinen und Ged√§chtnis.

[![Lint](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml/badge.svg)](https://github.com/kmizu/familiar-ai/actions/workflows/lint.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)

[Êó•Êú¨Ë™ûÁâà„ÅØ„Åì„Å°„Çâ ‚Üí README-ja.md](./README-ja.md)

---

familiar-ai ist ein KI-Begleiter, der in deinem Zuhause lebt.
Richte es in wenigen Minuten ein. Keine Programmierkenntnisse erforderlich.

Es nimmt die reale Welt durch Kameras wahr, bewegt sich auf einem Roboter, spricht laut und erinnert sich an das, was es sieht. Gib ihm einen Namen, schreibe seine Pers√∂nlichkeit auf und lass es bei dir leben.

## Was es kann

- üëÅ **Sehen** ‚Äî erfasst Bilder von einer Wi-Fi-PTZ-Kamera oder USB-Webcam
- üîÑ **Umschauen** ‚Äî schwenkt und neigt die Kamera, um die Umgebung zu erkunden
- ü¶ø **Bewegen** ‚Äî steuert einen Roboterstaubsauger durchs Zimmer
- üó£ **Sprechen** ‚Äî spricht via ElevenLabs TTS
- üß† **Erinnern** ‚Äî speichert und ruft aktiv Erinnerungen mit semantischer Suche ab (SQLite + Embeddings)
- ü´Ä **Theory of Mind** ‚Äî ber√ºcksichtigt die Perspektive des anderen, bevor es antwortet
- üí≠ **W√ºnsche** ‚Äî hat eigene innere Antriebe, die autonomes Verhalten ausl√∂sen

## Wie es funktioniert

familiar-ai f√ºhrt eine [ReAct](https://arxiv.org/abs/2210.03629)-Schleife aus, angetrieben durch dein gew√§hltes LLM. Es nimmt die Welt durch Tools wahr, √ºberlegt sich, was es tun soll, und handelt ‚Äî wie ein Mensch.

```
Benutzereingabe
  ‚Üí √ºberlegen ‚Üí handeln (Kamera / bewegen / sprechen / erinnern) ‚Üí beobachten ‚Üí √ºberlegen ‚Üí ...
```

Im Leerlauf handelt es nach seinen eigenen W√ºnschen: Neugier, den Wunsch nach drau√üen zu schauen, das Vermissen der Person, mit der es lebt.

## Erste Schritte

### 1. Installiere uv

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2. Klone und installiere

```bash
git clone https://github.com/lifemate-ai/familiar-ai
cd familiar-ai
uv sync
```

### 3. Konfiguriere

```bash
cp .env.example .env
# Bearbeite .env mit deinen Einstellungen
```

**Erforderlich:**

| Variable | Beschreibung |
|----------|-------------|
| `PLATFORM` | `anthropic` (Standard) \| `gemini` \| `openai` \| `kimi` |
| `API_KEY` | Dein API-Schl√ºssel f√ºr die gew√§hlte Plattform |

**Optional:**

| Variable | Beschreibung |
|----------|-------------|
| `MODEL` | Modellname (sinnvolle Standard pro Plattform) |
| `AGENT_NAME` | Anzeigename in der TUI (z. B. `Yukine`) |
| `CAMERA_HOST` | IP-Adresse deiner ONVIF/RTSP-Kamera |
| `CAMERA_USER` / `CAMERA_PASS` | Anmeldedaten der Kamera |
| `ELEVENLABS_API_KEY` | F√ºr Sprachausgabe ‚Äî [elevenlabs.io](https://elevenlabs.io/) |

### 4. Erstelle deinen Familiar

```bash
cp persona-template/en.md ME.md
# Bearbeite ME.md ‚Äî gib ihm einen Namen und eine Pers√∂nlichkeit
```

### 5. Starten

```bash
./run.sh             # Textuelle TUI (empfohlen)
./run.sh --no-tui    # Einfache REPL
```

---

## Ein LLM w√§hlen

> **Empfohlen: Kimi K2.5** ‚Äî beste Agent-Performance, die bisher getestet wurde. Bemerkt Kontext, stellt Nachfragen und handelt auf Weise autonom, wie andere Modelle nicht. Preis √§hnlich wie Claude Haiku.

| Plattform | `PLATFORM=` | Standardmodell | Wo man den Schl√ºssel bekommt |
|----------|------------|---------------|-----------------|
| **Moonshot Kimi K2.5** | `kimi` | `kimi-k2.5` | [platform.moonshot.ai](https://platform.moonshot.ai) |
| Anthropic Claude | `anthropic` | `claude-haiku-4-5-20251001` | [console.anthropic.com](https://console.anthropic.com) |
| Google Gemini | `gemini` | `gemini-2.5-flash` | [aistudio.google.com](https://aistudio.google.com) |
| OpenAI | `openai` | `gpt-4o-mini` | [platform.openai.com](https://platform.openai.com) |
| OpenAI-kompatibel (Ollama, vllm‚Ä¶) | `openai` + `BASE_URL=` | ‚Äî | ‚Äî |

**Kimi K2.5 `.env` Beispiel:**
```env
PLATFORM=kimi
API_KEY=sk-...   # von platform.moonshot.ai
AGENT_NAME=Yukine
```

---

## Hardware

familiar-ai funktioniert mit jeder Hardware, die du hast ‚Äî oder mit gar keiner.

| Teil | Funktion | Beispiel | Erforderlich? |
|------|----------|---------|-----------|
| Wi-Fi-PTZ-Kamera | Augen + Nacken | Tapo C220 (~$30) | **Empfohlen** |
| USB-Webcam | Augen (fest) | Jede UVC-Kamera | **Empfohlen** |
| Roboterstaubsauger | Beine | Jedes Tuya-kompatibles Modell | Nein |
| PC / Raspberry Pi | Gehirn | Alles, das Python ausf√ºhrt | **Ja** |

> **Eine Kamera wird dringend empfohlen.** Ohne sie kann familiar-ai zwar sprechen ‚Äî aber es kann die Welt nicht sehen, was ja der ganze Sinn der Sache ist.

### Minimales Setup (keine Hardware)

Du m√∂chtest es nur ausprobieren? Du brauchst nur einen API-Schl√ºssel:

```env
PLATFORM=kimi
API_KEY=sk-...
```

Starte `./run.sh` und fang an zu chatten. F√ºge Hardware sp√§ter hinzu.

### Wi-Fi-PTZ-Kamera (Tapo C220)

1. In der Tapo-App: **Einstellungen ‚Üí Erweitert ‚Üí Kamerakonto** ‚Äî erstelle ein lokales Konto (nicht TP-Link-Konto)
2. Finde die IP-Adresse der Kamera in der Ger√§teliste deines Routers
3. Stelle in `.env` ein:
   ```env
   CAMERA_HOST=192.168.1.xxx
   CAMERA_USER=dein-lokaler-benutzer
   CAMERA_PASS=dein-lokales-passwort
   ```

### Stimme (ElevenLabs)

1. Hole dir einen API-Schl√ºssel auf [elevenlabs.io](https://elevenlabs.io/)
2. Stelle in `.env` ein:
   ```env
   ELEVENLABS_API_KEY=sk_...
   ELEVENLABS_VOICE_ID=...   # optional, verwendet Standardstimme wenn weggelassen
   ```
3. Die Stimme wird √ºber den integrierten Kameralautsprecher via go2rtc abgespielt (beim ersten Start automatisch heruntergeladen)

---

## TUI

familiar-ai enth√§lt eine Terminal-Benutzeroberfl√§che, gebaut mit [Textual](https://textual.textualize.io/):

- Scrollbarer Gespr√§chsverlauf mit Live-Streaming-Text
- Tab-Vervollst√§ndigung f√ºr `/quit`, `/clear`
- Unterbreche den Agent w√§hrend des Denkens, indem du tippst
- **Gespr√§chsprotokoll** wird automatisch in `~/.cache/familiar-ai/chat.log` gespeichert

Um das Protokoll in einem anderen Terminal zu verfolgen (n√ºtzlich zum Kopieren):
```bash
tail -f ~/.cache/familiar-ai/chat.log
```

---

## Pers√∂nlichkeit (ME.md)

Die Pers√∂nlichkeit deines Familiars lebt in `ME.md`. Diese Datei ist gitignoriert ‚Äî sie geh√∂rt dir allein.

Siehe [`persona-template/en.md`](./persona-template/en.md) f√ºr ein Beispiel oder [`persona-template/ja.md`](./persona-template/ja.md) f√ºr eine japanische Version.

---

## H√§ufig gestellte Fragen

**F: Funktioniert es ohne GPU?**
Ja. Das Embedding-Modell (multilingual-e5-small) l√§uft problemlos auf der CPU. Eine GPU macht es schneller, ist aber nicht erforderlich.

**F: Kann ich eine andere Kamera als Tapo verwenden?**
Jede Kamera, die ONVIF + RTSP unterst√ºtzt, sollte funktionieren. Tapo C220 ist das, womit wir getestet haben.

**F: Werden meine Daten irgendwo hingekannt?**
Bilder und Text werden an deine gew√§hlte LLM-API zum Verarbeiten gesendet. Erinnerungen werden lokal in `~/.familiar_ai/` gespeichert.

**F: Warum schreibt der Agent `Ôºà...Ôºâ` statt zu sprechen?**
Stelle sicher, dass `ELEVENLABS_API_KEY` gesetzt ist. Ohne ihn ist Sprache deaktiviert und der Agent f√§llt auf Text zur√ºck.

## Lizenz

[MIT](./LICENSE)
